---
title: "Data Manipulation Challenge"
subtitle: "A Mental Model for Method Chaining in Pandas"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# ðŸ”— Data Manipulation Challenge - A Mental Model for Method Chaining in Pandas


## The Problem: Mastering Data Manipulation Through Method Chaining

**Core Question:** How can we efficiently manipulate datasets using `pandas` method chaining to answer complex business questions?

**The Challenge:** Real-world data analysis requires combining multiple data manipulation techniques in sequence. Rather than creating intermediate variables at each step, method chaining allows us to write clean, readable code that flows logically from one operation to the next.

**Our Approach:** We'll work with ZappTech's shipment data to answer critical business questions about service levels and cross-category orders, using the seven mental models of data manipulation through pandas method chaining.

## The Seven Mental Models of Data Manipulation

The seven most important ways we manipulate datasets are:

1. **Assign:** Add new variables with calculations and transformations
2. **Subset:** Filter data based on conditions or select specific columns
3. **Drop:** Remove unwanted variables or observations
4. **Sort:** Arrange data by values or indices
5. **Aggregate:** Summarize data using functions like mean, sum, count
6. **Merge:** Combine information from multiple datasets
7. **Split-Apply-Combine:** Group data and apply functions within groups


## Data and Business Context

We analyze ZappTech's shipment data, which contains information about product deliveries across multiple categories. This dataset is ideal for our analysis because:

- **Real Business Questions:** CEO wants to understand service levels and cross-category shopping patterns
- **Multiple Data Sources:** Requires merging shipment data with product category information
- **Complex Relationships:** Service levels may vary by product category, and customers may order across categories
- **Method Chaining Practice:** Perfect for demonstrating all seven mental models in sequence

## Data Loading and Initial Exploration

Let's start by loading the ZappTech shipment data and understanding what we're working with.

```{python}
#| label: load-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# Load the shipment data
shipments_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/shipments.csv", 
    parse_dates=['plannedShipDate', 'actualShipDate']
)

# Load product line data
product_line_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/productLine.csv"
)

# Reduce dataset size for faster processing (4,000 rows instead of 96,805 rows)
shipments_df = shipments_df.head(4000)

print("Shipments data shape:", shipments_df.shape)
print("\nShipments data columns:", shipments_df.columns.tolist())
print("\nFirst few rows of shipments data:")
print(shipments_df.head(10))

print("\n" + "="*50)
print("Product line data shape:", product_line_df.shape)
print("\nProduct line data columns:", product_line_df.columns.tolist())
print("\nFirst few rows of product line data:")
print(product_line_df.head(10))
```

::: {.callout-note}
## ðŸ’¡ Understanding the Data

**Shipments Data:** Contains individual line items for each shipment, including:
- `shipID`: Unique identifier for each shipment
- `partID`: Product identifier
- `plannedShipDate`: When the shipment was supposed to go out
- `actualShipDate`: When it actually shipped
- `quantity`: How many units were shipped

**Product Category and Line Data:** Contains product category information:
- `partID`: Links to shipments data
- `productLine`: The category each product belongs to
- `prodCategory`: The category each product belongs to

**Business Questions We'll Answer:**
1. Does service level (on-time shipments) vary across product categories?
2. How often do orders include products from more than one category?
:::

## The Seven Mental Models: A Progressive Learning Journey

Now we'll work through each of the seven mental models using method chaining, starting simple and building complexity.

### 1. Assign: Adding New Variables

**Mental Model:** Create new columns with calculations and transformations.

Let's start by calculating whether each shipment was late:

```{python}
#| label: mental-model-1-assign
#| echo: true

# Simple assignment - calculate if shipment was late
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days
    )
)

print("Added lateness calculations:")
print(shipments_with_lateness[['shipID', 'plannedShipDate', 'actualShipDate', 'is_late', 'days_late']].head())
```

::: {.callout-tip}
## ðŸ’¡ Method Chaining Tip for New Python Users

**Why use `lambda df:`?** When chaining methods, we need to reference the current state of the dataframe. The `lambda df:` tells pandas "use the current dataframe in this calculation." Without it, pandas would look for a variable called `df` that doesn't exist.

**Alternative approach:** You could also write this as separate steps, but method chaining keeps related operations together and makes the code more readable.
:::

::: {.callout-important}
## ðŸ¤” Discussion Questions: Assign Mental Model

**Question 1: Data Types and Date Handling**
- What is the `dtype` of the `actualShipDate` series? How can you find out using code?
- Why is it important that both `actualShipDate` and `plannedShipDate` have the same data type for comparison?


**Question 2: String vs Date Comparison**
- Can you give an example where comparing two dates as strings would yield unintuitive results, e.g. what happens if you try to compare "04-11-2025" and "05-20-2024" as strings vs as dates?

::: 


**Question 3: Debug This Code**
```python
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement="Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
    )
)
```

What's wrong with the `lateStatement` assignment and how would you fix it?


#### Briefly Give Answers to the Discussion Questions In This Section

1. 
`dtype` of `actualShipDate` is datetime64. 
```{python}
# | echo: True
# Find the dtype of 'actualShipDate'
print("Data type of 'actualShipDate':", shipments_df['actualShipDate'].dtype)

print("Data type of 'plannedShipDate'", shipments_df['plannedShipDate'].dtype)
```

2. 
It's important for `actualShipDate` and `plannedShipDate` to have the same data type, because as strings in lexicographic ordering, one might be greater than the other, but as date times it might be the opposite. 

For example: 

```{python}
# | echo: True

# Compare as strings
print('"04-11-2025" > "05-20-2024"?', "04-11-2025" > "05-20-2024")  # string comparison

from datetime import datetime

date1 = datetime.strptime("04-11-2025", "%m-%d-%Y")
date2 = datetime.strptime("05-20-2024", "%m-%d-%Y")
print('datetime(04-11-2025) > datetime(05-20-2024)?', date1 > date2)  # datetime comparison
```

3. 
The following code used incorrect syntax -- if/else isn't an operation that works on series/masks. The proper operation is .map().

```{python}
#| eval: false
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement=lambda df: df['is_late'].map({
            True: "Darn shipment is late",
            False: "Shipment is on time"
        }
        )
    )
)
```


### 2. Subset: Querying Rows and Filtering Columns

**Mental Model:** Query rows based on conditions and filter to keep specific columns.

Let's query for only late shipments and filter to keep the columns we need:

```{python}
#| label: mental-model-2-subset
#| echo: true

# Query rows for late shipments and filter to keep specific columns
late_shipments = (
    shipments_with_lateness
    .query('is_late == True')  # Query rows where is_late is True
    .filter(['shipID', 'partID', 'plannedShipDate', 'actualShipDate', 'days_late'])  # Filter to keep specific columns
)

print(f"Found {len(late_shipments)} late shipments out of {len(shipments_with_lateness)} total")
print("\nLate shipments sample:")
print(late_shipments.head())
```

::: {.callout-note}
## ðŸ” Understanding the Methods

- **`.query()`**: Query rows based on conditions (like SQL WHERE clause)
- **`.filter()`**: Filter to keep specific columns by name
- **Alternative**: You could use `.loc[]` for more complex row querying, but `.query()` is often more readable
:::

::: {.callout-important}
## ðŸ¤” Discussion Questions: Subset Mental Model

**Question 1: Query vs Boolean Indexing**
- What's the difference between using `.query('is_late == True')` and `[df['is_late'] == True]`?
- Which approach is more readable and why?

**Question 2: Additional Row Querying**
- Can you show an example of using a variable like `late_threshold` to query rows for shipments that are at least `late_threshold` days late, e.g. what if you wanted to query rows for shipments that are at least 5 days late?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
`.query('is_late == True')` is the method chaining way of writing a query operation. `[df['is_late'] == True]` is the traditional Pythonic way of doing it, but obscures the meaning behind the Python operations -- a subscription operator on the dataframe with `df['is_late']` and then a comparison operator `==`, which operationally interprets into "df rows, where is_late is True". The `.query()` method is far more explicit about this, since we can almost 1-1 interpret the keywords back into the natural language. 

2. 
```{python}
#| echo: true
# Query rows for shipments later than 5 days
late_shipments_5days = (
    shipments_with_lateness
    .assign(
        late_threshold=5
    )
    .query('days_late > late_threshold')  # Query rows where is_late is True
    .filter(['shipID', 'partID', 'plannedShipDate', 'actualShipDate', 'days_late'])  # Filter to keep specific columns
)

print(f"Found {len(late_shipments_5days)} shipments later than 5 days out of {len(shipments_with_lateness)} total")
print("\nLate shipments sample:")
print(late_shipments.head())
```



### 3. Drop: Removing Unwanted Data
 
**Mental Model:** Remove columns or rows you don't need.

Let's clean up our data by removing unnecessary columns:

```{python}
#| label: mental-model-3-drop
#| echo: true

# Create a cleaner dataset by dropping unnecessary columns
clean_shipments = (
    shipments_with_lateness
    .drop(columns=['quantity'])  # Drop quantity column (not needed for our analysis)
    .dropna(subset=['plannedShipDate', 'actualShipDate'])  # Remove rows with missing dates
)

print(f"Cleaned dataset: {len(clean_shipments)} rows, {len(clean_shipments.columns)} columns")
print("Remaining columns:", clean_shipments.columns.tolist())
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Drop Mental Model

**Question 1: Drop vs Filter Strategies**
- What's the difference between `.drop(columns=['quantity'])` and `.filter()` with a list of columns you want to keep?
- When would you choose to drop columns vs filter to keep specific columns?

**Question 2: Handling Missing Data**
- What happens if you use `.dropna()` without specifying `subset`? How is this different from `.dropna(subset=['plannedShipDate', 'actualShipDate'])`?
- Why might you want to be selective about which columns to check for missing values?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
They achieve mostly the same thing. However, if you have a particularly wide dataset and want to only drop a few, `.drop()` is more convenient than `.filter()`

2. 
By default, if you do not provide `.dropna()` with a `subset` argument, the default is that it drops all rows with a `null` in _any_ column. The default values for `.dropna()` per the documentation are:
- `subset=Null` -- this is interpreted as considering columns. 
- `how="any""` -- this is interpreted as "drop all rows that are missing any of the columns designated by `subset`"


### 4. Sort: Arranging Data

**Mental Model:** Order data by values or indices.

Let's sort by lateness to see the worst offenders:

```{python}
#| label: mental-model-4-sort
#| echo: true

# Sort by days late (worst first)
sorted_by_lateness = (
    clean_shipments
    .sort_values('days_late', ascending=False)  # Sort by days_late, highest first
    .reset_index(drop=True)  # Reset index to be sequential
)

print("Shipments sorted by lateness (worst first):")
print(sorted_by_lateness[['shipID', 'partID', 'days_late', 'is_late']].head(10))
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Sort Mental Model

**Question 1: Sorting Strategies**
- What's the difference between `ascending=False` and `ascending=True` in sorting?
- How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?

**Question 2: Index Management**
- Why do we use `.reset_index(drop=True)` after sorting?
- What happens to the original index when you sort? Why might this be problematic?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
`ascending=False` means that it is sorted in descending order of the designated column, in this case `days_late`. The highest value for `days_late`, i.e. the latest shipment, will be listed first.
To sort by multiple columns, the parameter `by` seems to accept type of `IndexLabel`, meaning we could pass in an array of columns or an index object like `df.columns`. 

2. 
We use `.reset_index(drop=True)` because when we sort, the row indices actually follow the original rows -- so the row indices will not be in increasing order as we expect.This method will also re-index, and discard the old one with `drop=True`. Normally it will preserve it in a new column.
Not resetting the index could be problematic if we use, say, `.loc[:5]` to take the top 5, when in reality indices 0, 1, 2, 3, and 4 could be anywhere in the dataframe.  

### 5. Aggregate: Summarizing Data

**Mental Model:** Calculate summary statistics across groups or the entire dataset.

Let's calculate overall service level metrics:

```{python}
#| label: mental-model-5-aggregate
#| echo: true

# Calculate overall service level metrics
service_metrics = (
    clean_shipments
    .agg({
        'is_late': ['count', 'sum', 'mean'],  # Count total, count late, calculate percentage
        'days_late': ['mean', 'max']  # Average and maximum days late
    })
    .round(3)
)

print("Overall Service Level Metrics:")
print(service_metrics)

# Calculate percentage on-time directly from the data
on_time_rate = (1 - clean_shipments['is_late'].mean()) * 100
print(f"\nOn-time delivery rate: {on_time_rate:.1f}%")
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Aggregate Mental Model

**Question 1: Boolean Aggregation**
- Why does `sum()` work on boolean values? What does it count?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
Running `sum()` on a Boolean series treats booleans as integers 0 and 1, so `sum()` just counts the number of `True` values. 

### 6. Merge: Combining Information

**Mental Model:** Join data from multiple sources to create richer datasets.

Now let's analyze service levels by product category. First, we need to merge our data:

```{python}
#| label: mental-model-6-merge-prep
#| echo: true

# Merge shipment data with product line data
shipments_with_category = (
    clean_shipments
    .merge(product_line_df, on='partID', how='left')  # Left join to keep all shipments
    .assign(
        category_late=lambda df: df['is_late'] & df['prodCategory'].notna()  # Only count as late if we have category info
    )
)

print("\nProduct categories available:")
print(shipments_with_category['prodCategory'].value_counts())
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Merge Mental Model

**Question 1: Join Types and Data Loss**
- Why does your professor think we should use `how='left'` in most cases? 
- How can you check if any shipments were lost during the merge?

**Question 2: Key Column Matching**
- What happens if there are duplicate `partID` values in the `product_line_df`?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
The idea here is to look up product information for each shipment using the field `partID`. Even if that `partID` field is null, we should keep that shipment, and that's what `how='left'` does. We can verify that no shipments were lost:

```{python}
print(f"Number of shipments in original dataset: {clean_shipments['actualShipDate'].count()}\n")
print(f"Number of shipments in new dataset: {shipments_with_category['actualShipDate'].count()}\n")

```

We use `actualShipDate` because this field is not tampered with when we join with the product dataset -- it doesn't exist in that one. If we have 4000 non-null `actualShipDate` rows then we know the original ones are in there. 


2. 
If there are duplicates on the right side, the row in the original dataset is duplicated also with respective product information for each duplicated row. 

### 7. Split-Apply-Combine: Group Analysis

**Mental Model:** Group data and apply functions within each group.

Now let's analyze service levels by category:

```{python}
#| label: mental-model-7-groupby
#| echo: true

# Analyze service levels by product category
service_by_category = (
    shipments_with_category
    .groupby('prodCategory')  # Split by product category
    .agg({
        'is_late': ['any', 'count', 'sum', 'mean'],  # Count, late count, percentage late
        'days_late': ['mean', 'max']  # Average and max days late
    })
    .round(3)
)

print("Service Level by Product Category:")
print(service_by_category)
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Split-Apply-Combine Mental Model

**Question 1: GroupBy Mechanics**
- What does `.groupby('prodCategory')` actually do? How does it "split" the data?
- Why do we need to use `.agg()` after grouping? What happens if you don't?

**Question 2: Multi-Level Grouping**
- Explore grouping by `['shipID', 'prodCategory']`?  What question does this answer versus grouping by `'prodCategory'` alone?  (HINT: There may be many rows with identical shipID's due to a particular order having multiple partID's.)
:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
`.groupby('prodCategory')` actually creates a Python GroupBy object, representing the original dataset subsetted out into different groups based on the value of `prodCategory`. It doesn't apply any summarization or calculate any metrics. If you don't do `.agg()`, you just have the regular GroupBy object which doesn't have any aggregation data, which still technically contains the raw data but in whatever way that the GroupBy object stores it. 


2. 
One `shipID` represents one shipment -- so this is far more granular. We are essentially looking at every order, grouping them up by all their shipments and seeing within _each_ shipment, which product categories are late and by how much? And how may of each category is shipped? 
```{python}
#| label: mental-model-7-groupby-2
#| echo: true

# Analyze service levels by product category
service_by_category = (
    shipments_with_category
    .groupby(['shipID','prodCategory'])  # Split by product category
    .agg({
        'is_late': ['any', 'count', 'sum', 'mean'],  # Count, late count, percentage late
        'days_late': ['mean', 'max']  # Average and max days late
    })
    .round(3)
)

print("Service Level by Product Category and Ship ID:")
print(service_by_category)
```



## Answering A Business Question

**Mental Model:** Combine multiple data manipulation techniques to answer complex business questions.

Let's create a comprehensive analysis by combining shipment-level data with category information:

```{python}
#| label: mental-model-7-comprehensive
#| echo: true

# Create a comprehensive analysis dataset
comprehensive_analysis = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])  # Group by shipment and category
    .agg({
        'is_late': 'any',  # True if any item in this shipment/category is late
        'days_late': 'max'  # Maximum days late for this shipment/category
    })
    .reset_index()
    .assign(
        has_multiple_categories=lambda df: df.groupby('shipID')['prodCategory'].transform('nunique') > 1
    )
)

print("Comprehensive analysis - shipments with multiple categories:")
multi_category_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]
print(f"Shipments with multiple categories: {multi_category_shipments['shipID'].nunique()}")
print(f"Total unique shipments: {comprehensive_analysis['shipID'].nunique()}")
print(f"Percentage with multiple categories: {multi_category_shipments['shipID'].nunique() / comprehensive_analysis['shipID'].nunique() * 100:.1f}%")
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Answering A Business Question

**Question 1: Business Question Analysis**
- What business question does this comprehensive analysis answer?
- How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?
- What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

1. 
This answers how many of the shipments shipped with multiple categories. We grouped by two fields `shipID` and `prodCategory`, and each have duplicate values in their columns because they have a many-to-many relationship. So we use `nunique()` to count the number of unique shipments from our multi-category-shipments-only dataset. 
Grouping by only `prodCategory` will destroy the `shipID` information in our dataset -- we would not be able to distinguish which shipment was associated to which product categories. 
The high percentage of multi-category shipments tells us that we should look further into whether one particular category is causing the whole shipment to be late. 

## Visualization: Service Level by Product Category
```{python}
#| label: service-level-by-category
#| echo: false
import matplotlib.pyplot as plt

# Calculate service level (on-time percentage) by product category
service_level = (
    shipments_with_category
    .groupby('prodCategory')['is_late']
    .apply(lambda x: 1 - x.mean())  # service level: 1 - fraction late
    .reset_index(name='service_level')
    .sort_values('service_level', ascending=False)
)

# Set up the horizontal bar plot
fig, ax = plt.subplots(figsize=(8,5))
bars = ax.barh(service_level['prodCategory'], service_level['service_level'], color='#1f77b4')

# Add percentage labels on bars, ensuring labels stay in bounds
for bar in bars:
    width = bar.get_width()
    yval = bar.get_y() + bar.get_height() / 2
    offset = -40 if width > 0.90 else 3  # move the label inside the bar for very high values
    ha = 'right' if width > 0.90 else 'left'
    ax.annotate(f"{width:.0%}",
                xy=(width, yval),
                xytext=(offset,0),
                textcoords="offset points",
                va='center', ha=ha, fontsize=11, color="white" if width > 0.90 else "black")

# Professional formatting
ax.set_title("Service Level by Product Category", fontsize=15, pad=13)
ax.set_ylabel("Product Category", fontsize=12)
ax.set_xlabel("On-Time Percentage", fontsize=12)
ax.set_xlim(0, 1)
ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))

plt.tight_layout()
plt.show()
```


::: {.callout-note}
## ðŸŽ¯ Method Chaining Philosophy

> "Each operation should build naturally on the previous one"

*Think of method chaining like building with LEGO blocks - each piece connects to the next, creating something more complex and useful than the individual pieces.*
:::
